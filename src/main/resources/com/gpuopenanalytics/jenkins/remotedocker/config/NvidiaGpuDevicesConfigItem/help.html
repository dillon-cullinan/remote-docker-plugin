<!--
  ~ Copyright (c) 2019, NVIDIA CORPORATION.
  ~
  ~ Licensed under the Apache License, Version 2.0 (the "License");
  ~ you may not use this file except in compliance with the License.
  ~ You may obtain a copy of the License at
  ~
  ~     http://www.apache.org/licenses/LICENSE-2.0
  ~
  ~ Unless required by applicable law or agreed to in writing, software
  ~ distributed under the License is distributed on an "AS IS" BASIS,
  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  ~ See the License for the specific language governing permissions and
  ~ limitations under the License.
  -->

<div>
    Define which GPU devices will be available in the main container.
    <ul>
        <li><strong>all</strong> - Every GPU on the system will be passed into the container. This is the default.</li>
        <li><strong>none</strong> - No GPUs will be passed into the container. Driver capabilities are still available, but no CUDA code can be executed.</li>
        <li><strong>void</strong> - Disables nvidia-docker</li>
        <li><strong>executor</strong> - Assuming the node has the same number of GPU devices as executors, this chooses the GPU index equal to the current executor. This is recommended to allow for concurrent builds or multiple projects that only require a single GPU.</li>
        <li><strong>custom</strong> - Specify the exact GPUs to pass in</li>
    </ul>
</div>
